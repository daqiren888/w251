# HW09


##  1. Using 1 node 

#### Baseline performance is 15 hours for 5 epochs
<img src="./analysis/1node_TB.png" width="500"/>

## 2. Using 2 nodes parallel and distributed pytorch computing
#### The performance of 2 nodes in parallel is 7.5 hours for 5 epochs, that was exactely 1/2 of the baseline performance of 1 node.
<img src="./analysis/2node_TB.png" width="500"/>


## 3. Using 3 nodes parallel and distributed pytorch computing
#### The performance of 3 nodes in parallel is 5 hours for 5 epochs, that was exactely 1/3 of the baseline performance of 1 node.
<img src="./analysis/3node_TB.png" width="500"/>



## 4. Using 3 nodes parallel and distributed pytorch computing
#### The performance of 3 nodes in parallel is 5 hours for 5 epochs, that was exactely 1/3 of the baseline performance of 1 node.
<img src="./analysis/P1.png" width="500"/>
<img src="./analysis/P2.png" width="500"/>
<img src="./analysis/P3.png" width="500"/>
<img src="./analysis/P4.png" width="500"/>
